{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "New Daily Data Dump Script",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9UazONJ3l6DSNYLVLqS8R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SophiaU/Git_DoD_Remittance/blob/main/New_Daily_Data_Dump_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMB-puNmtpk6"
      },
      "source": [
        "Package Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXjkjBSStnpx",
        "outputId": "b59cfa6f-17f4-44fe-d2d8-66703878115d"
      },
      "source": [
        "# install and import pygsheets\n",
        "!pip install pygsheets\n",
        "import pygsheets\n",
        "import gspread\n",
        "\n",
        "# import pandas\n",
        "import pandas as pd\n",
        "from functools import reduce\n",
        "\n",
        "#  Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pygsheets\n",
            "  Downloading pygsheets-2.0.5-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▏                             | 10 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 20 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 30 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 40 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 51 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 61 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 71 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 81 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 92 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 102 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 112 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 122 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 133 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 143 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 147 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.5.5 in /usr/local/lib/python3.7/dist-packages (from pygsheets) (1.12.8)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from pygsheets) (0.4.6)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.5.5->pygsheets) (1.26.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.5.5->pygsheets) (3.0.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.5.5->pygsheets) (1.15.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.5.5->pygsheets) (1.35.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.5.5->pygsheets) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.5.5->pygsheets) (0.0.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.5.5->pygsheets) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.5.5->pygsheets) (1.53.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.5.5->pygsheets) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.5.5->pygsheets) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.5.5->pygsheets) (3.17.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.5.5->pygsheets) (21.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.5.5->pygsheets) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.5.5->pygsheets) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.5.5->pygsheets) (4.2.4)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.5.5->pygsheets) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client>=1.5.5->pygsheets) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.5.5->pygsheets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.5.5->pygsheets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.5.5->pygsheets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.5.5->pygsheets) (2021.10.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->pygsheets) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pygsheets) (3.1.1)\n",
            "Installing collected packages: pygsheets\n",
            "Successfully installed pygsheets-2.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whlXokQAtwJb"
      },
      "source": [
        "### Authentications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GngixvvOtwth"
      },
      "source": [
        "##### Import The JSON File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "RqEjeToQvC6N",
        "outputId": "52ec4e57-5061-4dc1-d32c-46326a899a36"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9985e1e9-c2ae-4bee-be2d-ac42ff948a42\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9985e1e9-c2ae-4bee-be2d-ac42ff948a42\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving service_account.json to service_account.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1PIt7b6tydZ"
      },
      "source": [
        "#  Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# authenticate gspread\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "#authenticate pygsheets\n",
        "import json\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "with open('service_account.json') as source:\n",
        "   info = json.load(source)\n",
        "credentials = service_account.Credentials.from_service_account_info(info)\n",
        "\n",
        "client = pygsheets.authorize(service_account_file='service_account.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_mnhABQvIdF"
      },
      "source": [
        "### Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqUWJv8AvyNU"
      },
      "source": [
        "day = 'Nov 18'\n",
        "\n",
        "weekFolderID = '1BcYdBsK5UQYD6YUD5eOo5-8w_LP3D86l'\n",
        "weekFolder = drive.ListFile({'q': \"'\"+weekFolderID+\"' in parents and  mimeType = 'application/vnd.google-apps.folder'\"}).GetList()\n",
        "VAFolderID = '1cDvmmJBVmz4FPpePljiZ1clEMnbo9LPQ'\n",
        "\n",
        "lagos = \"(not title contains '(12)' and not title contains '(13)' and not title contains '(14)' and not title contains '(30)' and not title contains '(20)' and not title contains '(10)' and not title contains '(9)' and not title contains '(7)' and not title contains '(6)')\"\n",
        "dataDumpLagos = '1Q3BJcXwRncZiiB3GAYw1CNN-RWOWjt4AyUHxjs_Kztg'\n",
        "\n",
        "ghana = \"(title contains '(6)' or title contains '(9)')\"\n",
        "dataDumpGhana = '1bfBtlRbueriORkEYE4jXv4yHmFsOr_tdYZlGlVp8cJ0'\n",
        "\n",
        "sa = \"(title contains '(7)' or title contains '(10)' or title contains '(30)' or title contains '(20)')\"\n",
        "dataDumpSA = '1YDi41F705U9R4MBda0tILrkxR0uM8b9iX-MK5yombQQ'\n",
        "\n",
        "pd.set_option('display.max_columns', 500)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izlL8Bxu2N1n"
      },
      "source": [
        "Combine Driver Performance Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhPL2qyS2NQB"
      },
      "source": [
        "def combine_performance_files(country):\n",
        "  # Append all files in folder into a listframe\n",
        "\n",
        "  listframe = []\n",
        "\n",
        "\n",
        "  for folder in weekFolder:\n",
        "    if folder['title'] == 'Performance':\n",
        "      # https://developers.google.com/drive/v2/web/search-parameters\n",
        "      files = drive.ListFile({'q': \"title contains '\"+day+\"'and \"+country+\" and '\"+folder['id']+\"' in parents \\\n",
        "                              and mimeType = 'text/csv' and trashed = false \"}).GetList()\n",
        "      for file in files:\n",
        "        print(file['title'])\n",
        "        # download\n",
        "        fileDownloaded = drive.CreateFile({'id':file['id']})\n",
        "\n",
        "        # load\n",
        "        fileDownloaded.GetContentFile('performance.csv')\n",
        "\n",
        "        # read\n",
        "        df = pd.read_csv('performance.csv', delimiter=',' )\n",
        "\n",
        "        #Renaming\n",
        "        df = df.rename({'Time on trip (days: hours : minutes)':'Time on trip (days : hours : minutes)','Driver last name':'Driver surname'}, axis=1)\n",
        "\n",
        "        # append\n",
        "        listframe.append(df)\n",
        "\n",
        "  # combine all files in the list into a dataframe\n",
        "  dayperffile = pd.concat(listframe)\n",
        "\n",
        "# Driver UUID\tDriver first name\tDriver surname\tTrips completed\tTime online (days : hours: minutes)\tTime on trip (days : hours : minutes)\n",
        "\n",
        "  # convert hour online to just hours\n",
        "  dayperffile['Time online (days : hours: minutes)'] = round(pd.to_timedelta(dayperffile['Time online (days : hours: minutes)']).dt.total_seconds()/60,2)\n",
        "  dayperffile['Time on trip (days : hours : minutes)'] = round(pd.to_timedelta(dayperffile['Time on trip (days : hours : minutes)']).dt.total_seconds()/60,2)\n",
        "\n",
        "  # convert strings to numerics\n",
        "  columnslist = ['Trips completed', 'Time online (days : hours: minutes)', 'Time on trip (days : hours : minutes)']\n",
        "  for a in columnslist:\n",
        "    dayperffile[a] = pd.to_numeric(dayperffile[a],errors='coerce')\n",
        "\n",
        "  #Rename\n",
        "  dayperffile = dayperffile.rename({'Trips completed':'PerfTrips completed'}, axis=1)\n",
        "\n",
        "  # Combine first and last names\n",
        "  dayperffile['Name'] = dayperffile['Driver first name'] + \" \" + dayperffile['Driver surname']\n",
        "\n",
        "  # aggregate metrics by Driver UUID\n",
        "  dayperffile = dayperffile.groupby(['Driver UUID','Name']).agg({'PerfTrips completed':'sum','Time online (days : hours: minutes)':'sum', 'Time on trip (days : hours : minutes)':'sum'}).reset_index()\n",
        "\n",
        "  # print(dayperffile)\n",
        "  return dayperffile\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiMAhTln2H3X"
      },
      "source": [
        "Combine Driver Quality Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txRyXl992VL3"
      },
      "source": [
        "def combine_driverquality_files(country):\n",
        "  # Append all files in folder into a listframe\n",
        "\n",
        "  listframe = []\n",
        "\n",
        "\n",
        "  for folder in weekFolder:\n",
        "    if folder['title'] == 'Driver Quality':\n",
        "      # https://developers.google.com/drive/v2/web/search-parameters\n",
        "      files = drive.ListFile({'q': \"title contains '\"+day+\"'and \"+country+\" and '\"+folder['id']+\"' in parents \\\n",
        "                              and mimeType = 'text/csv' and trashed = false \"}).GetList()\n",
        "      for file in files:\n",
        "        print(file['title'])\n",
        "        # download\n",
        "        fileDownloaded = drive.CreateFile({'id':file['id']})\n",
        "\n",
        "        # load\n",
        "        fileDownloaded.GetContentFile('driver_quality.csv')\n",
        "\n",
        "        # read\n",
        "        df = pd.read_csv('driver_quality.csv', delimiter=',' )\n",
        "\n",
        "        # Rename Acceptance rate to Confirmation rate\n",
        "        df = df.rename({'Acceptance rate':'Confirmation rate','Driver last name':'Driver surname'}, axis=1)\n",
        "\n",
        "        # append\n",
        "        listframe.append(df)\n",
        "\n",
        "  # combine all files in the list into a dataframe\n",
        "  daydrvqualityfile = pd.concat(listframe)\n",
        "\n",
        "  \n",
        "# Driver UUID\tDriver first name\tDriver surname\tTrips completed\tConfirmation rate\tCancellation rate\tCompletion rate\tDriver ratings (last 4 weeks)\tDriver ratings (previous 500 trips)\n",
        "\n",
        "  # convert strings to numerics\n",
        "  columnslist = ['Trips completed', 'Confirmation rate', 'Cancellation rate', 'Completion rate', 'Driver ratings (last 4 weeks)', 'Driver ratings (previous 500 trips)']\n",
        "  for a in columnslist:\n",
        "    daydrvqualityfile[a] = pd.to_numeric(daydrvqualityfile[a],errors='coerce')\n",
        "\n",
        "  tripcompleted =daydrvqualityfile['Trips completed'].astype(str).str.split(\" \",n=0,expand=True)\n",
        "  daydrvqualityfile['Trips completed'] = tripcompleted[0].fillna(0)\n",
        "  daydrvqualityfile['Trips completed'] = pd.to_numeric(daydrvqualityfile['Trips completed'])\n",
        "\n",
        "  confirmationrate =daydrvqualityfile['Confirmation rate'].astype(str).str.split(\" \",n=0,expand=True)\n",
        "  daydrvqualityfile['Confirmation rate'] = confirmationrate[0].fillna(0)\n",
        "  daydrvqualityfile['Confirmation rate'] = pd.to_numeric(daydrvqualityfile['Confirmation rate'],errors='coerce')\n",
        "\n",
        "  cancellationrate =daydrvqualityfile['Cancellation rate'].astype(str).str.split(\" \",n=0,expand=True)\n",
        "  daydrvqualityfile['Cancellation rate'] = cancellationrate[0].fillna(0)\n",
        "  daydrvqualityfile['Cancellation rate'] = pd.to_numeric(daydrvqualityfile['Cancellation rate'],errors='coerce')\n",
        "\n",
        "  completionrate =daydrvqualityfile['Completion rate'].astype(str).str.split(\" \",n=0,expand=True)\n",
        "  daydrvqualityfile['Completion rate'] = cancellationrate[0].fillna(0)\n",
        "  daydrvqualityfile['Completion rate'] = pd.to_numeric(daydrvqualityfile['Completion rate'],errors='coerce')\n",
        "\n",
        "  daydrvqualityfile['Driver ratings (last 4 weeks)'] = pd.to_numeric(daydrvqualityfile['Driver ratings (last 4 weeks)'],errors='coerce')\n",
        "  daydrvqualityfile['Driver ratings (previous 500 trips)'] = pd.to_numeric(daydrvqualityfile['Driver ratings (previous 500 trips)'],errors='coerce')\n",
        "\n",
        "  # aggregate metrics by Uber ID\n",
        "  daydrvqualityfile = daydrvqualityfile.groupby('Driver UUID').agg({'Trips completed':'sum', 'Confirmation rate':'mean','Cancellation rate':'mean', 'Completion rate':'mean',\\\n",
        "                                              'Driver ratings (last 4 weeks)':'mean', 'Driver ratings (previous 500 trips)':'mean'}).reset_index()\n",
        "\n",
        "  # print(daydrvqualityfile)\n",
        "  return daydrvqualityfile\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlOvcif4RBnR"
      },
      "source": [
        "Combine Trip Activity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iILT4-nYQ_QV"
      },
      "source": [
        "def combine_tripactivity_files(country):\n",
        "  # Append all files in folder into a listframe\n",
        "\n",
        "  listframe = []\n",
        "\n",
        "\n",
        "  for folder in weekFolder:\n",
        "    if folder['title'] == 'Trip Activity':\n",
        "      # https://developers.google.com/drive/v2/web/search-parameters\n",
        "      files = drive.ListFile({'q': \"title contains '\"+day+\"'and \"+country+\" and '\"+folder['id']+\"' in parents \\\n",
        "                              and mimeType = 'text/csv' and trashed = false \"}).GetList()\n",
        "      for file in files:\n",
        "        print(file['title'])\n",
        "        # download\n",
        "        fileDownloaded = drive.CreateFile({'id':file['id']})\n",
        "\n",
        "        # load\n",
        "        fileDownloaded.GetContentFile('trip_activity.csv')\n",
        "\n",
        "        # read\n",
        "        df = pd.read_csv('trip_activity.csv', delimiter=',' )\n",
        "\n",
        "        # append\n",
        "        listframe.append(df)\n",
        "\n",
        "  # combine all files in the list into a dataframe\n",
        "  combinedtripactivityfile = pd.concat(listframe)\n",
        "\n",
        "  # convert strings to numerics\n",
        "  combinedtripactivityfile['Trip distance'] = pd.to_numeric(combinedtripactivityfile['Trip distance'],errors='coerce')\n",
        "\n",
        "  tripDistance =combinedtripactivityfile['Trip distance'].astype(str).str.split(\" \",n=0,expand=True)\n",
        "  combinedtripactivityfile['Trip distance'] = tripDistance[0].fillna(0)\n",
        "  combinedtripactivityfile['Trip distance'] = pd.to_numeric(combinedtripactivityfile['Trip distance'])\n",
        "\n",
        "  # pick only Uber ID and Trips Distance\n",
        "  daytripactivityfile =combinedtripactivityfile[['Driver UUID', 'Trip distance']]\n",
        "  # return daytripactivityfile\n",
        "\n",
        "  # get total trip_distance\n",
        "  # sum trip distance by Driver UUID\n",
        "  trip_distance = daytripactivityfile.groupby('Driver UUID').agg({'Trip distance':'sum'}).reset_index()\n",
        "  # trip_distance.columns = ['Driver UUID', 'Trip distance']\n",
        "\n",
        "  # print(trip_distance)\n",
        "\n",
        "  return trip_distance\n",
        "\n",
        "# Trip UUID\tDriver UUID\tDriver first name\tDriver surname\tVehicle UUID\tNumber plate\tService type\tTrip request time\tTrip drop-off time\tPick-up address\tDrop-off address\tTrip distance\tTrip status\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7q_4rCwdqsK"
      },
      "source": [
        "Combine Payments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbrAA87-Q-8u"
      },
      "source": [
        "def combine_payments_files(country):\n",
        "  # Append all files in folder into a listframe\n",
        "\n",
        "  listframe = []\n",
        "\n",
        "\n",
        "  for folder in weekFolder:\n",
        "    if folder['title'] == 'Payments':\n",
        "      # https://developers.google.com/drive/v2/web/search-parameters\n",
        "      files = drive.ListFile({'q': \"title contains '\"+day+\"'and \"+country+\" and '\"+folder['id']+\"' in parents \\\n",
        "                              and mimeType = 'text/csv' and trashed = false \"}).GetList()\n",
        "      for file in files:\n",
        "        print(file['title'])\n",
        "        # download\n",
        "        fileDownloaded = drive.CreateFile({'id':file['id']})\n",
        "\n",
        "        # load\n",
        "        fileDownloaded.GetContentFile('payments.csv')\n",
        "\n",
        "        # read\n",
        "        df = pd.read_csv('payments.csv', delimiter=',' )\n",
        "        df = df.rename({'Paid to you : Your earnings':'Total Earnings','Paid to you : Trip balance : Payouts : Cash collected':'Cash Collected',\n",
        "                        'Paid to you':'Uber Balance','Paid to you : Trip balance : Payouts : Cash Collected':'Cash Collected',\\\n",
        "                        'Paid to you : Your earnings : Fare':'Net Fares', 'Paid to you:Your earnings:Service fee':'Uber Fee',\\\n",
        "                        'Paid to you:Your earnings:Service Fee':'Uber Fee'}, axis=1)\n",
        "\n",
        "        # append\n",
        "        listframe.append(df)\n",
        "\n",
        "  # combine all files in the list into a dataframe\n",
        "  daypaymentsfile = pd.concat(listframe)\n",
        "\n",
        "# transaction UUID\tDriver UUID\tDriver first name\tDriver surname\tTrip UUID\tDescription\tOrganisation name\tOrg alias\tvs reporting\tPaid to you\tPaid to you : Your earnings\tPaid to you : Trip balance : Payouts : Cash collected\tPaid to you : Your earnings : Fare\tPaid to you : Your earnings : Taxes\tPaid to you:Your earnings:Fare:Fare\tPaid to you:Your earnings:Fare:Surge\tPaid to you:Your earnings:Service fee\tPaid to you:Your earnings:Fare:Wait Time at Pick-up\tPaid to you:Your earnings:You saved:Quest\tPaid to you:Your earnings:Fare:Cancellation\tPaid to you:Trip balance:Refunds:Airport fee\tPaid to you:Your earnings:Tip\tPaid to you:Trip balance:Refunds:Toll\tPaid to you:Trip balance:Payouts:Transferred To Bank Account\tPaid to you:Your earnings:Fare:Adjustment\n",
        "\n",
        "  # aggregate metrics by Driver UUID\n",
        "  daypaymentsfile = daypaymentsfile.groupby('Driver UUID').agg({'Total Earnings':'sum','Cash Collected':['sum', lambda x: (x < 0).sum()],'Uber Balance':'sum','Net Fares':'sum','Uber Fee':'sum' }).reset_index()\n",
        "  daypaymentsfile.columns = ['Driver UUID','Total Earnings','CashCollected','CashTrips', 'Uber Balance','Net Fares', 'Uber Fee']\n",
        "\n",
        "\n",
        "  # print(daypaymentsfile)\n",
        "  return daypaymentsfile\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXn9u67YkdeD"
      },
      "source": [
        "Get Moove Details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFEI5Vqokgiq"
      },
      "source": [
        "def moove_details(dumpsheet):\n",
        "  # get moove details\n",
        "  mooveDetailList =[]\n",
        "  worksheet = gc.open_by_key(dumpsheet).sheet1\n",
        "  for i in range(2,6):\n",
        "    columns = worksheet.col_values(i)\n",
        "    mooveDetailList.append(columns[1:])\n",
        "\n",
        "  # print(mooveDetailList)\n",
        "\n",
        "  # convert list to data frame\n",
        "  mooveDetails = pd.DataFrame(mooveDetailList)\n",
        "  mooveDetails = mooveDetails.T\n",
        "  mooveDetails.columns = ['Name','Product','DRN', 'Driver UUID']\n",
        "  mooveDetails['Name'] = pd.Series(mooveDetails['Name']).astype(str).str.title()\n",
        "\n",
        "  # print(mooveDetails)\n",
        "  return mooveDetails"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HmSySOJksva"
      },
      "source": [
        "#### Combine All Files and Moove Details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xm1GYCXkyMs"
      },
      "source": [
        "def generate_reconfile(driverquality_file,perffile,tripactivityfile,paymentsfile,mooveDetails):\n",
        "\n",
        "  # only unique driver names and uber IDs from Moove details\n",
        "  # mooveDetailsName = mooveDetails[['Driver UUID', 'Name']].drop_duplicates(subset='Name')\n",
        "\n",
        "  # join on driverUUID\n",
        "  # mooveDetailsID = mooveDetails[['Product','DRN', 'Driver UUID']].drop_duplicates(subset='Driver UUID')\n",
        "\n",
        "  dfs = [driverquality_file,perffile,tripactivityfile,paymentsfile]\n",
        "  reconfile = reduce(lambda left,right: pd.merge(left,right,how='left', on='Driver UUID'), dfs)\n",
        "\n",
        "  # wrangle reconfile\n",
        "  reconfile = reconfile.fillna(0)\n",
        "\n",
        "  reconfile['Date'] = day\n",
        "  reconfile['CashCollected'] = -reconfile['CashCollected']\n",
        "  reconfile['GrossFares'] = reconfile['Net Fares'] - reconfile['Uber Fee']\n",
        "  reconfile['GrossFT'] = round(reconfile['GrossFares']/reconfile['PerfTrips completed'],2)\n",
        "  reconfile['TripsAccepted'] = round(reconfile['PerfTrips completed']/(1-reconfile['Cancellation rate']),0)\n",
        "  reconfile['CardTrips'] = reconfile['PerfTrips completed']-reconfile['CashTrips']\n",
        "  reconfile['CardRevenue'] = reconfile['GrossFares'] - reconfile['CashCollected']\n",
        "\n",
        "  reconfile = reconfile[['Date','Driver UUID', 'Name','PerfTrips completed', 'Time online (days : hours: minutes)', 'Time on trip (days : hours : minutes)', \n",
        "                         'Confirmation rate', 'Cancellation rate', 'Completion rate', 'Driver ratings (last 4 weeks)', 'Driver ratings (previous 500 trips)',\n",
        "                         'Trip distance', 'Total Earnings', 'CashCollected', 'Uber Balance','Net Fares', 'Uber Fee', \n",
        "                         'GrossFares', 'GrossFT', 'TripsAccepted', 'CashTrips', 'CardTrips', 'CardRevenue']]\n",
        "  reconfile.drop_duplicates(subset=['Name', 'Driver UUID','PerfTrips completed'],inplace=True)\n",
        "\n",
        "  # reconfile.head(400)\n",
        "\n",
        "  return reconfile\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL8eZ5dCk26T"
      },
      "source": [
        "Write Data to Google Sheets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-_xY8mMk5VB"
      },
      "source": [
        "def write_to_sheets(reconfile,dumpsheet):\n",
        "  reconSheet = client.open_by_key(dumpsheet) \n",
        "  wksalldriversfile = reconSheet.worksheet_by_title(\"newDailyData\")\n",
        "  lastrow = len(wksalldriversfile.get_values('A1','A40000'))+1\n",
        "    \n",
        "  wksalldriversfile.set_dataframe(reconfile,(lastrow,1),copy_head=False,nan='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn5Are9wkHix"
      },
      "source": [
        "Lagos Data Dump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBUHG56zkIgb"
      },
      "source": [
        "def lagos_data_dump(country,dumpsheet):\n",
        "  driverquality_file = combine_driverquality_files(country)\n",
        "  perffile = combine_performance_files(country)\n",
        "  tripactivityfile = combine_tripactivity_files(country)\n",
        "  paymentsfile = combine_payments_files(country)\n",
        "  mooveDetails = moove_details(dumpsheet)\n",
        "\n",
        "  reconfile = generate_reconfile(driverquality_file,perffile,tripactivityfile,paymentsfile,mooveDetails)\n",
        "  write_to_sheets(reconfile,dumpsheet)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OeOB6W3kK5a"
      },
      "source": [
        "Ghana Data Dump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAxfYUkNkNW7"
      },
      "source": [
        "def ghana_data_dump(country,dumpsheet):\n",
        "  driverquality_file = combine_driverquality_files(country)\n",
        "  perffile = combine_performance_files(country)\n",
        "  tripactivityfile = combine_tripactivity_files(country)\n",
        "  paymentsfile = combine_payments_files(country)\n",
        "  mooveDetails = moove_details(dumpsheet)\n",
        "\n",
        "  reconfile = generate_reconfile(driverquality_file,perffile,tripactivityfile,paymentsfile,mooveDetails)\n",
        "  write_to_sheets(reconfile,dumpsheet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFulbUpIkQQD"
      },
      "source": [
        "SA Data Dump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okb8vS8dkTMB"
      },
      "source": [
        "def sa_data_dump(country,dumpsheet):\n",
        "  driverquality_file = combine_driverquality_files(country)\n",
        "  perffile = combine_performance_files(country)\n",
        "  tripactivityfile = combine_tripactivity_files(country)\n",
        "  paymentsfile = combine_payments_files(country)\n",
        "  mooveDetails = moove_details(dumpsheet)\n",
        "\n",
        "  reconfile = generate_reconfile(driverquality_file,perffile,tripactivityfile,paymentsfile,mooveDetails)\n",
        "  write_to_sheets(reconfile,dumpsheet)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abYUx0t8DimM"
      },
      "source": [
        "Run Data Dump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVfRNNv5Dl9j",
        "outputId": "24608851-e835-4431-f400-f0ead92ee366"
      },
      "source": [
        "# lagos_data_dump(lagos,dataDumpLagos)\n",
        "# ghana_data_dump(ghana,dataDumpGhana)\n",
        "sa_data_dump(sa,dataDumpSA)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "driver_quality Nov 18(20).csv\n",
            "driver_quality Nov 18(30).csv\n",
            "driver_quality Nov 18(10).csv\n",
            "driver_quality Nov 18(7).csv\n",
            "driver_performance Nov 18(20).csv\n",
            "driver_performance Nov 18(30).csv\n",
            "driver_performance Nov 18(10).csv\n",
            "driver_performance Nov 18(7).csv\n",
            "trip_activity Nov 18(20).csv\n",
            "trip_activity Nov 18(30).csv\n",
            "trip_activity Nov 18(10).csv\n",
            "trip_activity Nov 18(7).csv\n",
            "Payments Nov 18(20).csv\n",
            "Payments Nov 18(30).csv\n",
            "Payments Nov 18(10).csv\n",
            "Payments Nov 18(7).csv\n"
          ]
        }
      ]
    }
  ]
}